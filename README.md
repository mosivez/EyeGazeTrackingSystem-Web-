
**基于MediaPipe的Web眼动追踪系统 (EyeGazeTrackingWeb) 模块功能与系统使用说明报告**

**版本:** 1.0 (截至 2025-05-06)
**开发者:** @mosivez (在AI导师的协助下)

**目录**

1.  引言
2.  系统架构概览
3.  后端模块详细说明
    *   3.1. `config.py` - 配置模块
    *   3.2. `eye_detector.py` - 人脸与眼部特征检测模块
    *   3.3. `pose_estimator.py` - 头部姿态估计模块
    *   3.4. `gaze_mapper.py` - 视线特征提取与映射模块
    *   3.5. `calibration.py` - 校准模块
    *   3.6. `kalman_filter.py` - 数据平滑/滤波模块
    *   3.7. `evaluation.py` - 评估模块
    *   3.8. `visualization.py` - 可视化辅助模块
    *   3.9. `app.py` - 主应用与Web服务模块
4.  前端组件概览 (`templates/index.html`)
5.  系统安装与环境配置
    *   5.1. 依赖环境
    *   5.2. 获取代码与安装依赖
    *   5.3. **关键步骤：相机参数标定**
    *   5.4. 系统参数配置
6.  系统使用指南
    *   6.1. 启动系统
    *   6.2. 访问Web界面
    *   6.3. 校准流程
    *   6.4. 实时注视点跟踪
    *   6.5. 评估流程
    *   6.6. 眨眼检测
7.  常见问题与故障排除 (简要)
8.  未来展望与可扩展性
9.  结论

---

**1. 引言**

EyeGazeTrackingWeb 是一个基于 Python、MediaPipe、Flask 和 WebSocket 技术实现的实时眼动追踪系统。该系统通过摄像头捕捉用户面部信息，利用 MediaPipe 框架提取精确的面部及眼部关键点，结合头部姿态估计和用户校准数据，最终在屏幕上估计并显示用户的注视点。系统设计注重模块化、可维护性和用户交互性，旨在为相关研究和应用提供一个可扩展的基础平台。本报告将详细阐述各模块功能及系统使用方法。

**2. 系统架构概览**

本系统采用客户端-服务器 (Client-Server) 架构：

*   **后端 (Backend):** 使用 Python 和 Flask 框架构建。负责图像处理、关键点检测、姿态估计、校准模型训练、视线映射、数据滤波等核心逻辑。通过 Flask-SocketIO 实现与前端的实时双向通信。
*   **前端 (Frontend):** 使用 HTML, CSS, JavaScript 构建。负责展示来自后端的视频流、用户控制界面（校准、评估按钮）、实时状态信息（FPS、注视点坐标、头部姿态、眨眼次数等）、评估目标点显示及评估结果展示。

**3. 后端模块详细说明**

以下是构成系统后端的核心 Python 模块：

**3.1. `config.py` - 配置模块**

*   **职责:** 集中存储和管理系统的各类静态配置参数。
*   **主要参数:**
    *   摄像头ID、帧宽度/高度。
    *   MediaPipe Face Mesh 模型参数（最大人脸数、是否优化关键点、置信度阈值）。
    *   **相机内参矩阵 (`CAMERA_MATRIX`) 和畸变系数 (`DISTORTION_COEFFICIENTS`)**: **极其重要**，由 `camera_calibration.py` 脚本生成，用户必须用自己的相机标定结果替换。
    *   PnP 算法使用的面部3D模型点及对应2D关键点索引。
    *   校准流程参数（校准点坐标、每点采样数、模型保存路径）。
    *   卡尔曼滤波器参数（过程噪声、测量噪声）。
    *   眨眼检测参数（EAR阈值、连续帧数）。
    *   评估流程参数（评估目标点、观看者距离、屏幕物理高度）。
    *   Flask 和 SocketIO 服务器参数（主机、端口）。
*   **意义:** 方便用户根据自身硬件和需求调整系统行为，无需修改核心代码。

**3.2. `eye_detector.py` - 人脸与眼部特征检测模块**

*   **职责:** 利用 MediaPipe Face Mesh Solution 检测输入图像帧中的人脸，并精确定位面部（特别是眼睛区域）的关键点。同时，计算眼睛的开合度用于眨眼检测。
*   **核心技术:** MediaPipe Face Mesh。
*   **主要方法:**
    *   `__init__()`: 初始化 MediaPipe Face Mesh 模型，定义关键点索引。
    *   `process_frame()`: 处理单帧图像，返回 MediaPipe 检测结果和格式化的关键点列表。
    *   `get_eye_landmarks()`: 从完整关键点列表中提取左右眼角、虹膜中心等特定眼部关键点。
    *   `get_pnp_landmarks()`: 提取用于头部姿态估计 (PnP) 的特定面部关键点。
    *   `calculate_eye_opening_ratio()`: 根据眼睑关键点计算指定眼睛的开合度（EAR近似值）。
*   **输出:** 面部关键点坐标（归一化）、特定眼部关键点坐标、眼睛开合度。

**3.3. `pose_estimator.py` - 头部姿态估计模块**

*   **职责:** 利用检测到的面部2D关键点和预定义的3D面部模型点，估计头部的三维旋转（俯仰 Pitch、偏航 Yaw、翻滚 Roll）和平移向量。
*   **核心技术:** OpenCV `cv2.solvePnP()` 算法，基于2D-3D点对应关系。
*   **主要方法:**
    *   `__init__()`: 加载相机内参、畸变系数（来自 `config.py`），定义3D面部模型点。
    *   `estimate_pose()`: 输入2D关键点，执行 `solvePnP`，返回旋转向量、平移向量和计算得到的欧拉角。
*   **输出:** 头部旋转向量、平移向量、欧拉角（角度制）。

**3.4. `gaze_mapper.py` - 视线特征提取与映射模块**

*   **职责:**
    1.  提取与视线相关的特征，目前主要包括归一化的虹膜相对于眼角的位置，并可选择性地加入头部姿态角度作为附加特征。
    2.  在系统运行时，利用校准模块训练好的模型，将实时提取的特征映射为屏幕上的注视点坐标。
*   **主要方法:**
    *   `__init__()`: 初始化，尝试加载已训练的校准模型。
    *   `load_model()`: 从磁盘加载校准模型。
    *   `extract_features()`: 输入眼部关键点（来自`EyeDetector`）和可选的头部姿态角，计算并返回特征向量。
    *   `predict()`: 输入特征向量，使用加载的校准模型预测归一化的屏幕注视点坐标 (x, y)。
*   **输入:** 实时眼部关键点、实时头部姿态、校准模型。
*   **输出:** 原始（未滤波）的屏幕注视点坐标 (x, y)，范围通常为 [0, 1]。

**3.5. `calibration.py` - 校准模块**

*   **职责:** 实现用户个体化的眼动模型校准流程。收集用户在注视屏幕预定点时的眼部特征和头部姿态数据，并使用这些数据训练一个映射模型（如随机森林回归器）。
*   **核心技术:** 机器学习回归模型 (如 `sklearn.ensemble.RandomForestRegressor`)，特征缩放 (`StandardScaler`)，可选的超参数调优 (`GridSearchCV`)。
*   **主要方法:**
    *   `__init__()`: 初始化数据存储列表和模型对象。
    *   `add_data_point()`: 添加一组（特征向量, 目标屏幕坐标）到校准数据集中。
    *   `train_model()`: 使用收集的数据训练回归模型，包括特征缩放和可选的网格搜索调优。
    *   `save_model()`: 将训练好的模型（及缩放器）保存到磁盘。
    *   `load_model()`: 从磁盘加载之前保存的模型。
*   **输出:** 训练好的校准模型参数（保存为 `.pkl` 文件）。

**3.6. `kalman_filter.py` - 数据平滑/滤波模块**

*   **职责:** 对原始的、可能带有噪声和抖动的注视点估计结果进行滤波处理，以获得更稳定、平滑的注视点轨迹。
*   **核心技术:** 二维卡尔曼滤波器 (基于 `cv2.KalmanFilter`)。
*   **主要方法:**
    *   `__init__()`: 初始化卡尔曼滤波器参数（状态转移矩阵、测量矩阵、噪声协方差等）。
    *   `update()`: 输入新的测量值（原始注视点坐标），执行卡尔曼滤波的预测和校正步骤，返回平滑后的坐标。
    *   `reset()`: 重置滤波器状态。
*   **输入:** 原始注视点坐标序列。
*   **输出:** 平滑后的注视点坐标 (x, y)。

**3.7. `evaluation.py` - 评估模块**

*   **职责:** 提供量化评估系统注视点估计性能的工具和方法。
*   **主要方法:**
    *   `start_evaluation_session()`: 初始化评估会话，设置评估目标点序列。
    *   `get_current_target()`: 获取当前应注视的评估目标点。
    *   `record_sample()`: 记录一次评估样本（预测注视点, 实际目标点）。
    *   `next_target()`: 切换到下一个评估目标点。
    *   `calculate_metrics()`: 使用收集到的所有样本，计算多种误差指标，如平均绝对误差 (MAE)、均方根误差 (RMSE)，支持归一化坐标、像素坐标和近似角度误差。
*   **流程:** 用户在前端引导下注视特定点，按下按键记录样本，完成后系统计算并展示精度指标。

**3.8. `visualization.py` - 可视化辅助模块**

*   **职责:** (在当前Web版本中作用减弱，部分功能由前端HTML/CSS实现) 提供在OpenCV图像帧上绘制调试信息的函数，如面部网格、虹膜、头部姿态轴、注视点、校准/评估目标点、FPS、文本信息等。
*   **主要方法:** 包含多个 `draw_...()` 方法，用于在图像上绘制不同元素。
*   **当前用途:** 主要用于后端生成通过 `/video_feed` 推流的视频帧，上面可能还保留了一些基础绘制（如面部网格、姿态轴、FPS）。前端HTML负责绘制注视点和评估目标点。

**3.9. `app.py` - 主应用与Web服务模块**

*   **职责:**
    1.  初始化整个系统：摄像头、所有处理模块实例。
    2.  作为Flask Web服务器，提供HTTP服务（如提供 `index.html` 和视频流 `/video_feed`）。
    3.  作为SocketIO服务器，处理与前端的实时双向通信，包括：
        *   定期向前端发送系统状态更新（FPS、注视点、姿态、眨眼数、校准/评估状态等）。
        *   接收前端的控制命令（如开始/停止校准、开始/停止/记录评估样本）。
    4.  协调数据在各模块间的流动：读取摄像头帧 -> 特征检测 -> 姿态估计 -> （校准数据收集/视线映射） -> 滤波 -> 更新状态 -> （生成视频帧）。
    5.  管理系统的主状态（如是否正在校准 `is_calibrating`，是否正在评估 `is_evaluating`）。
*   **核心:** 整个系统的“指挥中心”和与用户交互的桥梁。

**4. 前端组件概览 (`templates/index.html`)**

前端通过单一HTML页面与用户交互，并动态更新信息：

*   **视频展示区 (`#video-feed`):** 通过HTTP流实时展示由后端处理和绘制的摄像头视频帧。
*   **注视点覆盖层 (`#gaze-overlay`, `#gaze-dot`):** 一个HTML `div` 元素，根据从后端收到的平滑后注视点坐标，通过JavaScript实时更新其在视频区域的位置，以红色圆点形式显示。
*   **评估目标点显示 (`#evaluation-html-target`):** 另一个HTML `div` 元素，在评估阶段，根据后端指令显示黄色的目标点，引导用户注视。
*   **控制面板 (`#controls`, `#evaluation-panel`):** 包含“开始/停止校准”、“开始/停止评估”按钮。按钮状态（启用/禁用）由JavaScript根据系统当前状态动态管理。
*   **状态显示面板 (`#status`):** 实时展示：
    *   WebSocket连接状态。
    *   摄像头帧率 (FPS)。
    *   系统当前运行状态（如：初始化、校准中、评估中、运行中等）。
    *   校准或评估过程中的提示信息。
    *   估计的注视点屏幕坐标 (X, Y)。
    *   头部姿态欧拉角 (Pitch, Yaw, Roll)。
    *   眨眼次数统计。
*   **评估结果区 (`#evaluation-results-area`):** 评估完成后，以格式化列表形式展示各项精度指标。
*   **JavaScript逻辑:**
    *   建立和管理与后端SocketIO服务器的连接。
    *   监听来自服务器的 `status_update`, `calibration_control_update`, `evaluation_control_update`, `evaluation_results_update` 事件，并据此更新页面DOM元素。
    *   向服务器发送用户操作事件（如 `start_calibration_event`, `capture_evaluation_sample_event`）。
    *   处理键盘事件（如按空格键记录评估样本）。
    *   动态管理按钮的可用性。

**5. 系统安装与环境配置**

**5.1. 依赖环境**

*   Python 3.8+
*   pip (Python包管理器)
*   一个可用的摄像头

**5.2. 获取代码与安装依赖**

1.  **获取代码:** (假设代码已存在于本地目录 `EyeGazeTrackingWeb/`)
2.  **创建并激活虚拟环境 (推荐):**
    ```bash
    cd EyeGazeTrackingWeb
    python -m venv venv
    # Windows:
    venv\Scripts\activate
    # macOS/Linux:
    source venv/bin/activate
    ```
3.  **安装Python依赖包:**
    ```bash
    pip install -r requirements.txt
    ```
    `requirements.txt` 文件应包含： `opencv-python`, `mediapipe`, `numpy`, `Flask`, `Flask-SocketIO`, `scikit-learn`, `joblib`, (可选 `eventlet` 或 `gevent` 用于生产环境SocketIO)。

**5.3. 关键步骤：相机参数标定**

这是保证头部姿态估计准确性的**前提**。

1.  **准备棋盘格:** 打印一个棋盘格图案（如9x6或8x5的内部角点），粘贴在硬平面板上。
2.  **运行标定脚本:**
    ```bash
    python camera_calibration.py
    ```
3.  **修改脚本参数:** 打开 `camera_calibration.py`，根据你的棋盘格修改 `CHESSBOARD_WIDTH` 和 `CHESSBOARD_HEIGHT` (内部角点数)。
4.  **采集图像:** 按照脚本窗口提示，在摄像头前从不同角度和距离展示棋盘格，按 'c' 键捕获图像，直到收集足够数量（如20张）。
5.  **获取结果:** 脚本运行完毕后，会在控制台打印出 `Camera Matrix (mtx)` 和 `Distortion Coefficients (dist)`，并保存到 `camera_calibration_results.json`。
6.  **更新配置:** **将打印出的 `mtx` 和 `dist` 的 NumPy 数组格式复制粘贴到 `config.py` 文件中，替换 `CAMERA_MATRIX` 和 `DISTORTION_COEFFICIENTS` 的占位符值。**

**5.4. 系统参数配置**

打开 `config.py` 文件，根据需要调整以下参数：

*   `CAMERA_ID`: 如果有多个摄像头，设置为正确的ID。
*   `EYE_AR_THRESH`, `EYE_AR_CONSEC_FRAMES_MIN_CLOSED`, `EYE_AR_CONSEC_FRAMES_MIN_OPEN`: 眨眼检测阈值，可能需要根据实际效果微调。
*   `VIEWER_DISTANCE_CM`, `SCREEN_HEIGHT_CM`: 用于评估模块计算近似角度误差，**务必设置为你的实际屏幕物理高度和通常的观看距离**。
*   其他参数如校准点、卡尔曼滤波参数等，可根据后续使用情况调整。

**6. 系统使用指南**

**6.1. 启动系统**

在激活虚拟环境的项目根目录下，运行主程序：

```bash
python app.py
```

服务器启动后，控制台会显示监听的地址和端口（默认为 `http://0.0.0.0:5000/` 或 `http://127.0.0.1:5000/`）。

**6.2. 访问Web界面**

打开现代浏览器（如 Chrome, Firefox），访问上述地址。应能看到系统界面和来自摄像头的视频流。

**6.3. 校准流程**

首次使用或更换用户/环境时，建议进行校准。

1.  **确保面部清晰可见:** 调整好坐姿和光线，确保面部完整且清晰地出现在摄像头画面中。
2.  **开始校准:** 点击界面上的 "开始校准" 按钮。
3.  **注视目标点:**
    *   视频画面中会出现一系列黄色的校准目标点（OpenCV绘制，如果已移除则前端无显示，后端依然按顺序进行）。
    *   **系统会自动按顺序进行，无需用户点击。** 用户需要做的就是当黄点出现时，**稳定地注视该黄色目标点**。
    *   系统会为每个目标点自动采集若干数据样本（由 `config.CALIBRATION_SAMPLES_PER_POINT` 定义）。
    *   前端状态区会显示当前正在校准的点和采集进度。
4.  **模型训练:** 所有校准点采集完毕后，后端会自动训练校准模型。此过程可能需要几秒钟。训练完成后，模型将自动保存到 `calibration_model.pkl`。
5.  **校准完成:** 前端状态区会提示校准完成。如果之前没有校准模型，系统现在将使用新训练的模型进行注视点估计。

**6.4. 实时注视点跟踪**

校准完成后（或已加载之前保存的校准模型），系统进入实时注视点跟踪模式。

*   **注视点显示:** 视频画面上会出现一个红色圆点（HTML绘制），代表系统估计的您的当前屏幕注视位置。尝试在屏幕范围内移动视线，观察红点是否跟随。
*   **信息解读:**
    *   **FPS:** 实时处理帧率，越高越流畅。
    *   **系统状态/消息:** 显示当前操作或提示。
    *   **注视点 (X, Y):** 归一化的屏幕坐标 (0-1范围)。
    *   **头部姿态 (P, Y, R):** 俯仰、偏航、翻滚角，可观察头部移动对结果的影响。

**6.5. 评估流程**

用于量化系统在当前条件下的注视点估计精度。

1.  **确保已校准:** 评估应在系统校准后进行。
2.  **开始评估:** 点击 "开始评估" 按钮。
3.  **注视HTML目标点:**
    *   视频画面区域会出现一个**黄色的HTML圆点**作为评估目标。
    *   前端状态区会提示当前是第几个评估目标。
    *   **稳定地注视此黄色HTML目标点**。
4.  **记录样本:** 当你确认自己正在准确注视目标点时，**按下键盘上的空格键**。后端会记录下你当前的注视点估计值和该目标点的真实坐标。
5.  **重复:** 黄色HTML目标点会自动移动到下一个预设位置。重复步骤3和4，直到所有评估点都记录完毕。
6.  **查看结果:** 所有点评估完毕后，前端会自动在 "评估结果" 区域显示详细的精度指标（MAE, RMSE等，包括归一化、像素和近似角度误差）。

**6.6. 眨眼检测**

*   **眨眼次数:** 前端状态区会实时显示检测到的眨眼次数。
*   **注视点行为:** 在检测到眨眼期间，注视点估计可能会暂停更新或红点消失，以避免因眼部关键点不可靠而导致的错误输出。

**7. 常见问题与故障排除 (简要)**

*   **摄像头无法打开/无视频流:**
    *   检查 `config.py` 中的 `CAMERA_ID` 是否正确。
    *   确保摄像头未被其他程序占用。
    *   检查摄像头驱动和物理连接。
*   **跟踪不准确/注视点漂移严重:**
    *   **首要检查：相机参数标定！** 确保 `config.py` 中的 `CAMERA_MATRIX` 和 `DISTORTION_COEFFICIENTS` 是使用你当前摄像头和设置（分辨率）准确标定得到的。
    *   重新进行用户校准。
    *   确保光线充足且均匀，避免面部过暗或过曝，避免强烈背光。
    *   保持头部相对稳定，尽量正对摄像头。
    *   尝试调整 `config.py` 中的卡尔曼滤波器参数或眨眼检测阈值。
*   **WebSocket 连接失败/状态不更新:**
    *   检查浏览器控制台是否有 WebSocket 连接错误信息。
    *   确保后端 `app.py` 成功启动且 SocketIO 服务正常运行。
    *   检查网络连接和防火墙设置。
*   **评估结果差:**
    *   除了上述跟踪不准确的原因外，确保评估时用户确实在认真注视目标点，并在视线稳定后才按空格记录。
    *   检查 `config.py` 中 `VIEWER_DISTANCE_CM` 和 `SCREEN_HEIGHT_CM` 设置是否准确，这会影响角度误差的计算。

**8. 未来展望与可扩展性**

*   **个性化用户模型:** 保存和加载针对不同用户的校准模型。
*   **自适应阈值:** 为眨眼检测或特征提取实现自适应阈值调整。
*   **更高级的校准方法:** 如隐式校准、动态校准。
*   **3D视线向量估计:** 而非仅屏幕2D点，可用于与3D环境交互。
*   **平滑追随算法:** 针对动态目标的注视点跟踪优化。
*   **生产环境部署:** 使用 `gunicorn` + `eventlet`/`gevent` 部署 Flask-SocketIO 应用。

**9. 结论**

EyeGazeTrackingWeb 系统成功地集成并实现了基于MediaPipe的人脸与眼部关键点检测、头部姿态估计、用户校准、实时视线映射、卡尔曼滤波平滑、眨眼检测以及性能评估等核心功能，并通过Web界面提供了友好的用户交互。模块化的设计为后续的进一步研究和功能扩展奠定了良好基础。通过本报告的指引，用户应能顺利配置、运行并有效使用本眼动追踪系统。

---

希望这份报告能满足你的要求，并对你的毕业设计有所帮助！如果你需要针对特定部分进行更深入的阐述，或者有其他问题，随时告诉我。
